#!/usr/bin/env python
# coding: utf-8

# # üîπ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –∏ –ø–æ–¥–±–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

# 1. –∏–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫

# In[2]:


# –†–∞–±–æ—Ç–∞ —Å —Ñ–∞–π–ª–∞–º–∏ –∏ –º–∞—Å—Å–∏–≤–∞–º–∏
import os
import numpy as np

import joblib

# –û–±—Ä–∞–±–æ—Ç–∫–∞ –∞—É–¥–∏–æ (–∑–∞–≥—Ä—É–∑–∫–∞ wav + MFCC)
import librosa

# –†–∞–∑–º–µ—Ç–∫–∞ –∏ –æ–±—É—á–µ–Ω–∏–µ
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV

# –ú–æ–¥–µ–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier

# –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
from sklearn.metrics import (
    classification_report,
    accuracy_score,
    confusion_matrix
)
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score


# 2. –ø—É—Ç–∏

# In[3]:


# –ö–æ—Ä–Ω–µ–≤–∞—è –ø–∞–ø–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
DATA_DIR = "cats_dogs"

# –ü–∞–ø–∫–∏ —Å train –∏ test
TRAIN_DIR = os.path.join(DATA_DIR, "train")
TEST_DIR  = os.path.join(DATA_DIR, "test")


# 3. –∏–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –ø–æ–º–æ—â—å—é MFCC

# In[4]:


def extract_features_mfcc(
    file_path,
    n_mfcc=30,
    n_fft=2048,
    hop_length=512,
    add_delta=True
):
    """
    –ß–∏—Ç–∞–µ—Ç –∞—É–¥–∏–æ-—Ñ–∞–π–ª –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ MFCC.

    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
    - n_mfcc: —á–∏—Å–ª–æ MFCC –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤ (–æ–±—ã—á–Ω–æ 13‚Äì40).
    - n_fft: —Ä–∞–∑–º–µ—Ä –æ–∫–Ω–∞ –ë–ü–§ (–±–æ–ª—å—à–µ -> —Ç–æ—á–Ω–µ–µ –ø–æ —á–∞—Å—Ç–æ—Ç–µ, –Ω–æ —Ç—è–∂–µ–ª–µ–µ).
    - hop_length: —à–∞–≥ –æ–∫–Ω–∞ (–º–µ–Ω—å—à–µ -> –ª—É—á—à–µ –ø–æ –≤—Ä–µ–º–µ–Ω–∏, –Ω–æ –±–æ–ª—å—à–µ –∫–∞–¥—Ä–æ–≤).
    - add_delta: –¥–æ–±–∞–≤–ª—è—Ç—å –ª–∏ –¥–µ–ª—å—Ç–∞-–ø—Ä–∏–∑–Ω–∞–∫–∏ (–∏–∑–º–µ–Ω–µ–Ω–∏–µ MFCC –ø–æ –≤—Ä–µ–º–µ–Ω–∏).
    """
    y, sr = librosa.load(file_path, sr=None)
    mfcc = librosa.feature.mfcc(
        y=y,
        sr=sr,
        n_mfcc=n_mfcc,
        n_fft=n_fft,
        hop_length=hop_length
    )

    mfcc_mean = np.mean(mfcc, axis=1)
    mfcc_std  = np.std(mfcc, axis=1)

    features = list(mfcc_mean) + list(mfcc_std)

    if add_delta:
        delta = librosa.feature.delta(mfcc)
        delta_mean = np.mean(delta, axis=1)
        delta_std  = np.std(delta, axis=1)

        delta2 = librosa.feature.delta(mfcc, order=2)
        delta2_mean = np.mean(delta2, axis=1)
        delta2_std  = np.std(delta2, axis=1)

        features += list(delta_mean) + list(delta_std)
        features += list(delta2_mean) + list(delta2_std)

    return np.array(features)


# 4. —Å–±–æ—Ä–∫–∞ X_train y_train X_test y_test

# In[5]:


def load_dataset(folder):
    """
    –û–±—Ö–æ–¥–∏—Ç –≤—Å–µ –ø–æ–¥–ø–∞–ø–∫–∏ –≤ `folder`, —Å—á–∏—Ç–∞–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ .wav
    –∏ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –º–∞—Ç—Ä–∏—Ü—É X –∏ –≤–µ–∫—Ç–æ—Ä y.
    """
    X = []
    y = []

    for label in os.listdir(folder):
        label_path = os.path.join(folder, label)
        if not os.path.isdir(label_path):
            continue
        for fname in os.listdir(label_path):
            file_path = os.path.join(label_path, fname)

            # –∏–∑–≤–ª–µ–∫–∞–µ–º MFCC-–ø—Ä–∏–∑–Ω–∞–∫–∏
            features = extract_features_mfcc(file_path)

            X.append(features)
            y.append(label)

    return np.array(X), np.array(y)

X_train, y_train = load_dataset(TRAIN_DIR)
X_test,  y_test  = load_dataset(TEST_DIR)


# 5. –∫–æ–¥–∏—Ä—É–µ–º –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Å–æ–≤ –≤ —á–∏—Å–ª–∞ (–º–æ–¥–µ–ª—è–º —É–¥–æ–±–Ω–µ–µ —Ä–∞–±–æ—Ç–∞—Ç—å —Å —á–∏—Å–ª–∞–º–∏ (0,1) –∞ –Ω–µ —Å–æ —Å—Ç—Ä–æ–∫–∞–º–∏)

# In[6]:


encoder = LabelEncoder()

# –û–±—É—á–∞–µ–º —ç–Ω–∫–æ–¥–µ—Ä –Ω–∞ –æ–±—É—á–∞—é—â–µ–π —Ä–∞–∑–º–µ—Ç–∫–µ
y_train_enc = encoder.fit_transform(y_train)

# –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–±—É—á–µ–Ω–Ω—ã–π —ç–Ω–∫–æ–¥–µ—Ä –∫ —Ç–µ—Å—Ç–æ–≤–æ–π —Ä–∞–∑–º–µ—Ç–∫–µ
y_test_enc = encoder.transform(y_test)

joblib.dump(encoder, "encoder.pkl");


# 6. –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏

# In[7]:


scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)

# –ü—Ä–∏–º–µ–Ω—è–µ–º scaler –∫ test
X_test = scaler.transform(X_test)

joblib.dump(scaler, "scaler.pkl")

X_train[2].shape


# 7. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π + –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
# 
# 1) LogisticRegression
# 2) RandomForest

# 1) LogisticRegression

# In[8]:


log_reg = LogisticRegression(
    C=1.0, # [0.001, 0.01, 0.1, 1, 10, 100, 1000]
    penalty='l2', # l1 –∏–ª–∏ l2
    solver='lbfgs', # 'liblinear', 'lbfgs', 'saga'
    max_iter=1000  # [500, 1000, 2000]
)


log_reg.fit(X_train, y_train_enc)
y_pred_lr = log_reg.predict(X_test)


# In[9]:


import warnings
from sklearn.exceptions import ConvergenceWarning
warnings.filterwarnings("ignore", category=ConvergenceWarning)
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)


# –ø–æ–¥–±–æ—Ä –ª—É—á—à–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ c –ø–æ–º–æ—â—å—é GridSearchCV (—Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º –º–Ω–æ–∂–µ—Å—Ç–≤–æ –≤–∞—Ä–∏–∞—Ü–∏–π —Ç–∫ –Ω–∞ —á–µ–º–ø–µ –¥–∞–∂–µ —Ç—ã—Å—è—á–Ω—ã–µ –≤–∞–∂–Ω—ã –≤–∞–∂–Ω—ã)

# In[10]:


from sklearn.model_selection import GridSearchCV
param_grid_lr = [
    # 1. lbfgs (—Ç–æ–ª—å–∫–æ l2, —Å–∞–º—ã–π –±—ã—Å—Ç—Ä—ã–π)
    {
        'penalty': ['l2'],
        'C': [0.001, 0.01, 0.1, 1, 10, 100],
        'solver': ['lbfgs'],
        'max_iter': [1000, 2000],
        'fit_intercept': [True],
        'class_weight': ['balanced', None]
    },

    # 2. liblinear (l1/l2, –Ω–æ dual –¢–û–õ–¨–ö–û –¥–ª—è l2)
    {
        'penalty': ['l1'],
        'C': [0.001, 0.01, 0.1, 1, 10, 100],
        'solver': ['liblinear'],
        'max_iter': [1000, 2000],
        'fit_intercept': [True],
        'class_weight': ['balanced', None],
        'dual': [False]  # l1 –ù–ï –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç dual=True
    },
    {
        'penalty': ['l2'],
        'C': [0.001, 0.01, 0.1, 1, 10, 100],
        'solver': ['liblinear'],
        'max_iter': [1000, 2000],
        'fit_intercept': [True],
        'class_weight': ['balanced', None],
        'dual': [False, True]  # l2 –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç dual
    },

    # 3. saga (–≤—Å–µ —Ç–∏–ø—ã penalty)
    {
        'penalty': ['l1', 'l2'],
        'C': [0.001, 0.01, 0.1, 1, 10, 100],
        'solver': ['saga'],
        'max_iter': [1000, 2000],
        'fit_intercept': [True],
        'class_weight': ['balanced', None]
    },
    {
        'penalty': ['elasticnet'],
        'C': [0.001, 0.01, 0.1, 1, 10, 100],
        'solver': ['saga'],
        'max_iter': [1000, 2000],
        'fit_intercept': [True],
        'class_weight': ['balanced', None],
        'l1_ratio': [0.1, 0.5, 0.9]
    }
]

grid_lr = GridSearchCV(
    LogisticRegression(),
    param_grid_lr,
    cv=3,
    scoring='f1_macro',
    n_jobs=-1,  
    verbose=1, 
    error_score=0.0  
)


grid_lr.fit(X_train, y_train_enc)


print(grid_lr.best_params_)
print(" –õ—É—á—à–∏–π F1:", grid_lr.best_score_)

best_lr = grid_lr.best_estimator_
y_pred_lr = best_lr.predict(X_test)


# 2) RandomForest 

# In[11]:


rf = RandomForestClassifier(
    n_estimators=200,   # —á–∏—Å–ª–æ –¥–µ—Ä–µ–≤—å–µ–≤
    max_depth=None,  # –≥–ª—É–±–∏–Ω–∞ –Ω–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∞
    random_state=42,
    n_jobs=-1 # –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤—Å–µ —è–¥—Ä–∞
)

rf.fit(X_train, y_train_enc)
y_pred_rf = rf.predict(X_test)


# –ø–æ–¥–±–æ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

# In[12]:


param_grid_rf = {
    "n_estimators": [100, 200, 500],
    "max_depth": [None, 10, 20, 30],
    "min_samples_split": [2, 5, 10],
    "min_samples_leaf": [1, 2, 4],
    "max_features": ["sqrt", "log2"]
}

grid_rf = GridSearchCV(
    estimator=RandomForestClassifier(random_state=42),
    param_grid=param_grid_rf,
    cv=3,
    scoring="f1_macro",
    n_jobs=-1
)

grid_rf.fit(X_train, y_train_enc)

print(grid_rf.best_params_)
print("–õ—É—á—à–∏–π F1:", grid_rf.best_score_)

best_rf = grid_rf.best_estimator_
y_pred_rf = best_rf.predict(X_test)


# 7. –ú–µ—Ç—Ä–∏–∫–∏ 

# In[18]:


y_pred_rf = best_rf.predict(X_test)
y_pred_lr = best_lr.predict(X_test)

print("LR Accuracy:", round(accuracy_score(y_test_enc, y_pred_lr), 3))
print("LR F1:", round(f1_score(y_test_enc, y_pred_lr, average='macro'), 3))
print()
print("RF Accuracy:", round(accuracy_score(y_test_enc, y_pred_rf), 3))
print("RF F1:", round(f1_score(y_test_enc, y_pred_rf, average='macro'), 3))


# —Ç–∞–∫–∂–µ –º–æ–∂–Ω–æ –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª–∏ —Ç–∞–∫–∏–µ –∫–∞–∫:

# In[19]:


from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import GradientBoostingClassifier

# 1. SVM
svm = SVC(random_state=42)
svm.fit(X_train, y_train_enc)
y_pred_svm = svm.predict(X_test)

# 2. Decision Tree
dt = DecisionTreeClassifier(random_state=42)
dt.fit(X_train, y_train_enc)
y_pred_dt = dt.predict(X_test)

# 3. Naive Bayes  
nb = GaussianNB()
nb.fit(X_train, y_train_enc)
y_pred_nb = nb.predict(X_test)

# 4. KNN
knn = KNeighborsClassifier()
knn.fit(X_train, y_train_enc)
y_pred_knn = knn.predict(X_test)

# 5. LDA
lda = LinearDiscriminantAnalysis()
lda.fit(X_train, y_train_enc)
y_pred_lda = lda.predict(X_test)

# 6. Gradient Boosting
gb = GradientBoostingClassifier(random_state=42)
gb.fit(X_train, y_train_enc)
y_pred_gb = gb.predict(X_test)


# –Ω–∞ –Ω–∞—à–µ–º –ø—Ä–∏–º–µ—Ä–µ –±–µ–∑ –ø–æ–¥–±–æ—Ä–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤–∏–¥–∏–º —Ç–∞–∫–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:

# In[20]:


models = {
    'SVM': y_pred_svm,
    'DT': y_pred_dt,
    'NB': y_pred_nb,
    'KNN': y_pred_knn,
    'LDA': y_pred_lda,
    'GB': y_pred_gb
}

for name, y_pred in models.items():
    acc = accuracy_score(y_test_enc, y_pred)
    f1 = f1_score(y_test_enc, y_pred, average='macro')
    print(f"{name}: Acc={acc:.3f} F1={f1:.3f}")


# # üîπ –ü–æ —á–∞—Å—Ç–æ—Ç–∞–º

# In[21]:


import numpy as np
import librosa
import os

# –§—É–Ω–∫—Ü–∏—è —á–∞—Å—Ç–æ—Ç—ã
def get_freq(file_path):
    y, sr = librosa.load(file_path)
    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=1)
    return np.mean(mfcc)

cat_freqs = []
dog_freqs = []

for file in os.listdir('cats_dogs/train/cat'):
    freq = get_freq(f'cats_dogs/train/cat/{file}')
    cat_freqs.append(freq)

for file in os.listdir('cats_dogs/train/dog'):
    freq = get_freq(f'cats_dogs/train/dog/{file}')
    dog_freqs.append(freq)

cat_avg = np.mean(cat_freqs)
dog_avg = np.mean(dog_freqs)
threshold = (cat_avg + dog_avg) / 2

print(f"–ö–æ—à–∫–∏: {cat_avg:.1f}")
print(f"–°–æ–±–∞–∫–∏: {dog_avg:.1f}")
print(f"–ü–æ—Ä–æ–≥: {threshold:.1f}")


# —Å—á–∏—Ç–∞–µ–º –Ω–∞ —Ç–µ—Å—Ç–µ

# In[22]:


correct = 0
total = 0

for label in ['cat', 'dog']:
    for file in os.listdir(f'cats_dogs/test/{label}'):
        if file.endswith('.wav'):
            freq = get_freq(f'cats_dogs/test/{label}/{file}')
            pred = 'dog' if freq > threshold else 'cat'

            if pred == label:
                correct += 1
            total += 1

print(f"\nAccuracy: {correct}/{total} = {correct/total:.3f}")


# ##

# # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
# 
# –ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ –º–æ–¥—É–ª—é 3 –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –º—ã —Å–æ—Ö—Ä–∞–Ω–∏–º –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫—É ```joblib```

# In[23]:


joblib.dump(best_rf, 'audio_model.pkl');


# In[40]:


joblib.dump(X_train, 'old_X_audio.pkl');
joblib.dump(y_train_enc, 'old_y_audio.pkl');
joblib.dump(X_test, "X_test_audio.pkl");
joblib.dump(y_test_enc, "y_test_audio.pkl");


# # –§—É–Ω–∫—Ü–∏—è –¥–æ–æ–±—É—á–µ–Ω–∏—è

# In[44]:


def fine_tuning_audio(new_audio_paths, new_labels) -> dict:
    """
    –ö–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π fine-tuning –∞—É–¥–∏–æ-–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞.
    –ú–æ–¥–µ–ª—å –¥–æ–æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–∏ —Å—Ç–∞—Ä—ã—Ö + –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ç–æ–≥–æ –∂–µ scaler, encoder –∏ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ test-–¥–µ–∫–∞.
    """

    # -------------------------------------------------
    # 1. –ó–ê–ì–†–£–ó–ö–ê –°–¢–ê–†–´–• –ê–†–¢–ï–§–ê–ö–¢–û–í
    # -------------------------------------------------
    scaler = joblib.load("scaler.pkl")
    encoder = joblib.load("encoder.pkl")

    old_X = joblib.load("old_X_audio.pkl")      # numpy array
    old_y = joblib.load("old_y_audio.pkl")      # numpy array

    model = joblib.load("audio_model.pkl")      # sklearn –º–æ–¥–µ–ª—å

    # -------------------------------------------------
    # 2. –ü–û–î–ì–û–¢–û–í–ö–ê –ù–û–í–´–• –î–ê–ù–ù–´–•
    # -------------------------------------------------
    X_new = []
    for path in new_audio_paths:
        feats = extract_features_mfcc(path)  # –¢–≤–æ—è —Ñ—É–Ω–∫—Ü–∏—è MFCC ‚Üí vector
        X_new.append(feats)

    X_new = np.array(X_new)

    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –º–µ—Ç–∫–∏ —á–µ—Ä–µ–∑ —Å—Ç–∞—Ä—ã–π encoder
    y_new = encoder.transform(new_labels)

    # –ø—Ä–∏–º–µ–Ω—è–µ–º —Ç–æ—Ç –∂–µ scaler, —á—Ç–æ –∏ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏
    X_new_scaled = scaler.transform(X_new)

    # -------------------------------------------------
    # 3. –û–ë–™–ï–î–ò–ù–ï–ù–ò–ï –°–¢–ê–†–´–• + –ù–û–í–´–• –î–ê–ù–ù–´–•
    # -------------------------------------------------
    X_full = np.vstack([old_X, X_new_scaled])
    y_full = np.concatenate([old_y, y_new])

    # -------------------------------------------------
    # 4. –ü–û–í–¢–û–†–ù–û–ï –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò
    # -------------------------------------------------
    model.fit(X_full, y_full)

    # -------------------------------------------------
    # 5. –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ù–ê –ó–ê–ì–†–£–ñ–ï–ù–ù–û–ú (joblib) TEST –ù–ê–ë–û–†–ï
    # -------------------------------------------------
    X_test = joblib.load("X_test_audio.pkl")
    y_test = joblib.load("y_test_audio.pkl")

    y_pred = model.predict(X_test)

    # -------------------------------------------------
    # 6. –†–ê–°–ß–Å–¢ –ú–ï–¢–†–ò–ö
    # -------------------------------------------------
    metrics = {
        "accuracy": round(accuracy_score(y_test, y_pred), 4),
        "f1_macro": round(f1_score(y_test, y_pred, average="macro"), 4),
        "precision_macro": round(precision_score(y_test, y_pred, average="macro"), 4),
        "recall_macro": round(recall_score(y_test, y_pred, average="macro"), 4),
    }

    # -------------------------------------------------
    # 7. –°–û–•–†–ê–ù–ï–ù–ò–ï –û–ë–ù–û–í–õ–Å–ù–ù–û–ô –ú–û–î–ï–õ–ò –ò TRAIN –î–ê–ù–ù–´–•
    # -------------------------------------------------
    joblib.dump(model, "audio_model.pkl")

    joblib.dump(X_full, "old_X_audio.pkl")
    joblib.dump(y_full, "old_y_audio.pkl")

    return metrics


# In[45]:


# –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é

X_new = []
y_new = []

for label in os.listdir(TRAIN_DIR):
    label_path = os.path.join(TRAIN_DIR, label)
    for fname in os.listdir(label_path):
        file_path = os.path.join(label_path, fname)

        X_new.append(file_path)
        y_new.append(label)


fine_tuning_audio(X_new, y_new)

