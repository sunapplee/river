#!/usr/bin/env python
# coding: utf-8

# # –ú–æ–¥—É–ª—å –ø–æ—Å–≤—è—â–µ–Ω –æ–±—É—á–µ–Ω–∏—é –º–æ–¥–µ–ª–∏ –∏ –µ–µ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å—É.
# 
# 
# –û–±—É—á–∞—Ç—å –±—É–¥–µ—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é –∏ —Ä–µ–≥—Ä–µ—Å—Å–∏—é. –û–±—ã—á–Ω–æ –≤ 3 –º–æ–¥—É–ª—é –∫–∞–∫ —Ä–∞–∑ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∑–∞–¥–∞—á.

# # –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫

# In[4]:


import matplotlib.pyplot as plt
import seaborn as sns

import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

from sklearn.linear_model import LinearRegression
from catboost import CatBoostRegressor
from sklearn.neural_network import MLPRegressor

from sklearn.metrics import r2_score, root_mean_squared_error, mean_absolute_error

from sklearn.linear_model import LogisticRegression
from catboost import CatBoostClassifier
from sklearn.neural_network import MLPClassifier

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.preprocessing import label_binarize
from sklearn.metrics import roc_curve, auc

from sklearn.model_selection import RandomizedSearchCV

from imblearn.over_sampling import SMOTE

import joblib


# # –ò–º–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö
# 
# –ú—ã –ø—Ä–µ–¥–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–ª–∏ –¥–∞–Ω–Ω—ã–µ –≤ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –º–æ–¥—É–ª—è—Ö, –ø–æ—ç—Ç–æ–º—É —Å –¥–∞–Ω–Ω—ã–º–∏ —Ä–∞–±–æ—Ç–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω–æ.

# In[5]:


df = pd.read_parquet('data/data.parquet')

df.head()


# In[6]:


X = df.drop(['AQI'], axis=1)
y = df['AQI']


# In[7]:


# –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –î–û –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)


# In[8]:


# –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º –¢–û–õ–¨–ö–û –ø–æ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–º –¥–∞–Ω–Ω—ã–º
scaler = StandardScaler()
X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)
X_test  = pd.DataFrame(scaler.transform(X_test),  columns=X_test.columns)


# In[9]:


joblib.dump(scaler, 'scaler.pkl')


# # –ú–æ–¥–µ–ª—å —Ä–µ–≥—Ä–µ—Å–∏–∏

# ## –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
# 
# 
# –î–ª—è –æ–±—É—á–µ–Ω–∏—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –±–µ—Ä–µ–º 3 –º–æ–¥–µ–ª–∏: –õ–∏–Ω–µ–π–Ω—É—é —Ä–µ–≥—Ä–µ—Å—Å–∏—é, –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –±—É—Å—Ç–∏–Ω–≥ (–∏–ª–∏ –°–ª—É—á–∞–π–Ω—ã–π –ª–µ—Å) –∏ –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å (–≤ –∫—Ä–∏—Ç–µ—Ä–∏—è—Ö –º–æ–∂–µ—Ç –±—ã—Ç—å –æ—Ç–¥–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–∫–∏, –ø–æ—ç—Ç–æ–º—É –±–µ—Ä–µ–º –≤—Å–µ–≥–¥–∞).
# 
# 
# –¢–∞–∫–æ–π –Ω–∞–±–æ—Ä –º–æ–¥–µ–ª–µ–π –æ–±—ä—è—Å–Ω—è–µ—Ç—Å—è –∏—Ö —Ä–∞–∑–Ω–æ–ø–ª–∞–Ω–æ–≤–æ—Å—Ç—å—é, —á—Ç–æ –ª—É—á—à–µ —Å–∫–∞–∂–µ—Ç—Å—è –Ω–∞ –∏—Ç–æ–≥–æ–≤–æ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ.

# ### –õ–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è

# In[10]:


model_lr = LinearRegression()
model_lr.fit(X_train, y_train)

y_pred_lr = model_lr.predict(X_test)


# ### –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –±—É—Å—Ç–∏–Ω–≥

# In[11]:


model_cb = CatBoostRegressor(
    verbose=0,
    # task_type='GPU'
)

model_cb.fit(X_train, y_train)
y_pred_cb = model_cb.predict(X_test)


# ### –ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å

# In[12]:


model_nn = MLPRegressor(
    hidden_layer_sizes=(64, 32),
    activation="relu",
    max_iter=1000,
    random_state=42
)

model_nn.fit(X_train, y_train)
y_pred_nn = model_nn.predict(X_test)


# ## –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
# 
# 1. MAE ‚Äî Mean Absolute Error
# 
# 
# 2. RMSE ‚Äî Root Mean Squared Error
# 
# 
# 3. R¬≤ ‚Äî –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–µ—Ç–µ—Ä–º–∏–Ω–∞—Ü–∏–∏

# In[13]:


print('MAE - —á–µ–º –º–µ–Ω—å—à–µ, —Ç–µ–º –ª—É—á—à–µ')
print(f'MAE –¥–ª—è LinearRegression: {mean_absolute_error(y_test, y_pred_lr):.3f}')
print(f'MAE –¥–ª—è CatBoostRegressor: {mean_absolute_error(y_test, y_pred_cb):.3f}')
print(f'MAE –¥–ª—è MLPRegressor: {mean_absolute_error(y_test, y_pred_nn):.3f}')


# In[14]:


print('RMSE - —á–µ–º –º–µ–Ω—å—à–µ, —Ç–µ–º –ª—É—á—à–µ')
print(f'RMSE –¥–ª—è LinearRegression: {root_mean_squared_error(y_test, y_pred_lr):.3f}')
print(f'RMSE –¥–ª—è CatBoostRegressor: {root_mean_squared_error(y_test, y_pred_cb):.3f}')
print(f'RMSE –¥–ª—è MLPRegressor: {root_mean_squared_error(y_test, y_pred_nn):.3f}')


# In[15]:


print('R2 - —á–µ–º –≤—ã—à–µ, —Ç–µ–º –ª—É—á—à–µ')
print(f'R2 –¥–ª—è LinearRegression: {r2_score(y_test, y_pred_lr):.3f}')
print(f'R2 –¥–ª—è CatBoostRegressor: {r2_score(y_test, y_pred_cb):.3f}')
print(f'R2 –¥–ª—è MLPRegressor: {r2_score(y_test, y_pred_nn):.3f}')


# –õ—É—á—à–µ–º–∏ –º–æ–¥–µ–ª—è–º–∏ —Å—Ç–∞–ª–∞ CatBoostRegressor –∏ –ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å, –ø–æ–¥–±–µ—Ä–µ–º –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –Ω–∏—Ö —Å –ø–æ–º–æ—â—å—é RandomizedSearchCV.

# ## Catboost —Å –ø–æ–¥–æ–±—Ä–∞–Ω–Ω—ã–º–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏.
# 
# –î–ª—è –ø–æ–¥–±–æ—Ä–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ Catboost –Ω—É–∂–Ω–æ –æ—Ç–∫–∞—Ç–∏—Ç—å—Å—è –¥–æ scikit-learn==1.5.2
# 
# –ï—Å–ª–∏ –≤–æ–∑–Ω–∏–∫–∞—é—Ç –ø—Ä–æ–±–ª–µ–º—ã, –º–æ–∂–Ω–æ –ø—Ä–æ—Å—Ç–æ –∑–∞–º–µ–Ω–∏—Ç—å –º–æ–¥–µ–ª—å Catboost –Ω–∞ RandomForest, —Å –Ω–∏–º —Ç–∞–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º –Ω–µ –±—É–¥–µ—Ç.
# 
# –û–¥–Ω–∞–∫–æ, –¥–ª—è –Ω–∞—á–∞–ª–∞ –ª—É—á—à–µ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –±—É—Å—Ç–∏–Ω–≥, —Ç–∞–∫ –∫–∞–∫ –æ–Ω –≤—ã–¥–∞–µ—Ç –æ–±—ã—á–Ω–æ –∫–∞—á–µ—Å—Ç–≤–æ –ª—É—á—à–µ.

# In[14]:


get_ipython().system('pip install scikit-learn==1.5.2 -q')


# In[15]:


param_dist_cb = {
    "depth": [4, 6, 8, 10],
    "learning_rate": [0.01, 0.03, 0.05, 0.1],
    "iterations": [300, 500, 800],
    "l2_leaf_reg": [1, 3, 5, 7, 9]
}

cb = CatBoostRegressor(verbose=0,
                      # task_type='GPU'
                      )

random_cb = RandomizedSearchCV(
    cb,
    param_distributions=param_dist_cb,
    cv=3,
    # n_jobs=-1,
    verbose=1,
    n_iter=3
)

random_cb.fit(X_train, y_train)

best_cb = random_cb.best_estimator_
y_pred_cb = best_cb.predict(X_test)

print(f'MAE –¥–ª—è CatBoostRegressor: {mean_absolute_error(y_test, y_pred_cb):.3f}')
print(f'RMSE –¥–ª—è CatBoostRegressor: {root_mean_squared_error(y_test, y_pred_cb):.3f}')
print(f'R2 –¥–ª—è CatBoostRegressor: {r2_score(y_test, y_pred_cb):.3f}')


# ## –ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å —Å –ø–æ–¥–æ–±—Ä–∞–Ω–Ω—ã–º–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏.

# In[16]:


param_grid_nn = {
    "hidden_layer_sizes": [(32, ), (64, ), (64, 32), (128, 64), (128, 128)],
    "activation": ["relu", "tanh"],
    "learning_rate_init": [0.001, 0.005, 0.01]
}

model_nn = MLPRegressor()


random_nn = RandomizedSearchCV(
    model_nn,
    param_distributions=param_grid_nn,
    cv=3,
    # n_jobs=-1,
    verbose=1,
    n_iter=3
)

random_nn.fit(X_train, y_train)

best_nn = random_nn.best_estimator_
y_pred_nn = best_nn.predict(X_test)

print(f'MAE –¥–ª—è MLPRegressor: {mean_absolute_error(y_test, y_pred_nn):.3f}')
print(f'RMSE –¥–ª—è MLPRegressor: {root_mean_squared_error(y_test, y_pred_nn):.3f}')
print(f'R2 –¥–ª—è MLPRegressor: {r2_score(y_test, y_pred_nn):.3f}')


# –õ—É—á—à–µ–π –º–æ–¥–µ–ª—å—é —Å—Ç–∞–ª–∞ CatboostRegressor —Å –ø–æ–¥–æ–±—Ä–∞–Ω–Ω—ã–º–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏. –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–æ–≥–Ω–æ–∑ –º–æ–¥–µ–ª–∏ —Å —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏.

# ## –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
# 

# In[17]:


import matplotlib.pyplot as plt

plt.scatter(y_test, y_pred_cb, alpha=0.6)
plt.plot([y_test.min(), y_test.max()],
         [y_test.min(), y_test.max()],
         linestyle="--")
plt.xlabel("Actual")
plt.ylabel("Predicted")
plt.title("Actual vs Predicted")
plt.show()


# –í —Ü–µ–ª–æ–º –ø–æ –≥—Ä–∞—Ñ–∏–∫—É –≤–∏–¥–Ω–∞ –æ—Ç–ª–∏—á–Ω–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è –º–µ–∂–¥—É –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ–º –∏ —Ä–µ–∞–ª—å–Ω–∞–º–∏ –¥–∞–Ω–Ω—ã–º–∏.

# ## –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
# 
# –î–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π sklearn –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∏–±–ª–∏–æ—Ç–µ–∫—É joblib. –•–æ—Ç—è –¥–ª—è catboost-–º–æ–¥–µ–ª–µ–π —ç—Ç–æ—Ç –º–µ—Ç–æ–¥ —Ç–æ–∂–µ –ø–æ–¥—Ö–æ–¥–∏—Ç, –ª—É—á—à–µ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–æ–¥–µ–ª—å —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏.

# In[18]:


best_cb.save_model('catboost_model_regression.cbm', format='cbm')

joblib.dump(best_nn, 'nn_model_regression.pkl');


# In[20]:


# ===========================================
# üü© –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –±—É–¥—É—â–µ–≥–æ fine-tuning
# ===========================================

joblib.dump(X_train, 'old_X_train_reg.pkl')
joblib.dump(y_train, 'old_y_train_reg.pkl')

joblib.dump(X_test, 'X_test_reg.pkl')
joblib.dump(y_test, 'y_test_reg.pkl')


# # –ú–æ–¥–µ–ª—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏

# ## –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö

# In[22]:


# –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º —Ä–µ–≥—Ä–µ—Å—Å–∏—é –≤ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é
# y ‚Äî —Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–π —Ç–∞—Ä–≥–µ—Ä
# labels = ["low", "medium", "high"]
bins = np.quantile(y, [0, 0.22, 0.89, 1.0])
labels = [0, 1, 2]

y_class = pd.cut(y, bins=bins, labels=labels, include_lowest=True)


# –í–∏–¥–∏–º, —á—Ç–æ –∫–ª–∞—Å—Å—ã –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã, –∫–ª–∞—Å—Å 1 –≤ 6 —Ä–∞–∑ –±–æ–ª—å—à–µ —á–µ–º –∫–ª–∞—Å—Å 2.

# In[23]:


y_class.value_counts(normalize=True) * 100


# In[24]:


X_train, X_test, y_train, y_test = train_test_split(
    X, y_class, test_size=0.2, random_state=42
)


# In[25]:


import seaborn as sns
import matplotlib.pyplot as plt

sns.countplot(x=y_class)
plt.title("Class distribution")
plt.show()


# ## –£—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞
# 
# –î–ª—è —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–µ—Ç–æ–¥ SMOTE, –æ–Ω —Å–æ–∑–¥–∞—Å—Ç —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –Ω–æ–≤—ã–µ –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è —Ä–µ–¥–∫–æ–≥–æ –∫–ª–∞—Å—Å–∞ –≤ –Ω–∞—à–µ–º –¥–∞—Ç–∞—Å–µ—Ç–µ.
# 
# 
# –î–µ–ª–∞–µ—Ç —ç—Ç–æ —Ç–∞–∫:
# 
# 1. –≤—ã–±–∏—Ä–∞–µ—Ç —Å–ª—É—á–∞–π–Ω—ã–π –æ–±—ä–µ–∫—Ç —Ä–µ–¥–∫–æ–≥–æ –∫–ª–∞—Å—Å–∞
# 
# 2. –≤—ã–±–∏—Ä–∞–µ—Ç –µ–≥–æ —Å–æ—Å–µ–¥–µ–π
# 
# 3. –∏–Ω—Ç–µ—Ä–ø–æ–ª–∏—Ä—É–µ—Ç (–≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–æ—á–∫–∏ –º–µ–∂–¥—É –Ω–∏–º–∏)

# In[26]:


smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)


# –ü–æ—Å–ª–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ–≤–µ—Ä–∏–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–æ–≤.

# In[27]:


y_train_resampled.value_counts(normalize=True) * 100


# In[28]:


import seaborn as sns
import matplotlib.pyplot as plt

sns.countplot(x=y_train_resampled)
plt.title("Class distribution")
plt.show()


# –í–∏–¥–∏–º, —á—Ç–æ –ø–æ—Å–ª–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–µ—Ç–æ–¥–∞ SMOTE –±–∞–ª–∞–Ω—Å –∏–¥–µ–∞–ª–µ–Ω, —á—Ç–æ –ø–æ–º–æ–∂–µ—Ç –ª—É—á—à–µ –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –∏ –≤—ã—á–∏—Å–ª–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏.

# ## –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
# 
# –ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏, –±–µ—Ä–µ–º —Ä–∞–∑–Ω–æ—Å—Ç–æ—Ä–æ–Ω–∏–µ –º–µ—Ç–æ–¥—ã –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.

# ### –õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è

# In[29]:


log_reg = LogisticRegression()

log_reg.fit(X_train_resampled, y_train_resampled)
y_pred_lr = log_reg.predict(X_test)


# ### –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –±—É—Å—Ç–∏–Ω–≥

# In[30]:


cb = CatBoostClassifier(verbose=0)

cb.fit(X_train_resampled, y_train_resampled)
y_pred_cb = cb.predict(X_test)


# ### –ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å

# In[31]:


nn = MLPClassifier()

nn.fit(X_train_resampled, y_train_resampled)
y_pred_nn = nn.predict(X_test)


# ## –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
# 
# 
# –î–ª—è multi-class –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏, –∫–∞–∫ –≤ –Ω–∞—à–µ–º —Å–ª—É—á–∞–µ, –ø–æ–¥–æ–π–¥—É—Ç:
# 
# - Accuracy
# 
# - Precision / Recall / F1 (–≤ —Å—Ä–µ–¥–Ω–µ–º)
# 
# - ROC/AUC (–º–∞–∫—Ä–æ –∏–ª–∏ micro)
# 
# - Confusion Matrix

# In[32]:


print("Accuracy ‚Äî —á–µ–º –≤—ã—à–µ, —Ç–µ–º –ª—É—á—à–µ")
print(f"Accuracy –¥–ª—è LogisticRegression: {accuracy_score(y_test, y_pred_lr):.3f}")
print(f"Accuracy –¥–ª—è CatBoostClassifier: {accuracy_score(y_test, y_pred_cb):.3f}")
print(f"Accuracy –¥–ª—è MLPClassifier: {accuracy_score(y_test, y_pred_nn):.3f}")


# In[33]:


print("F1 (macro) ‚Äî —á–µ–º –≤—ã—à–µ, —Ç–µ–º –ª—É—á—à–µ")
print(f"F1-macro –¥–ª—è LogisticRegression: {f1_score(y_test, y_pred_lr, average='macro'):.3f}")
print(f"F1-macro –¥–ª—è CatBoostClassifier: {f1_score(y_test, y_pred_cb, average='macro'):.3f}")
print(f"F1-macro –¥–ª—è MLPClassifier: {f1_score(y_test, y_pred_nn, average='macro'):.3f}")


# In[34]:


print("Precision (macro) ‚Äî —á–µ–º –≤—ã—à–µ, —Ç–µ–º –ª—É—á—à–µ")
print(f"Precision-macro –¥–ª—è LogisticRegression: {precision_score(y_test, y_pred_lr, average='macro'):.3f}")
print(f"Precision-macro –¥–ª—è CatBoostClassifier: {precision_score(y_test, y_pred_cb, average='macro'):.3f}")
print(f"Precision-macro –¥–ª—è MLPClassifier: {precision_score(y_test, y_pred_nn, average='macro'):.3f}")


# In[35]:


print("Recall (macro) ‚Äî —á–µ–º –≤—ã—à–µ, —Ç–µ–º –ª—É—á—à–µ")
print(f"Recall-macro –¥–ª—è LogisticRegression: {recall_score(y_test, y_pred_lr, average='macro'):.3f}")
print(f"Recall-macro –¥–ª—è CatBoostClassifier: {recall_score(y_test, y_pred_cb, average='macro'):.3f}")
print(f"Recall-macro –¥–ª—è MLPClassifier: {recall_score(y_test, y_pred_nn, average='macro'):.3f}")


# In[36]:


from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

cm_lr = confusion_matrix(y_test, y_pred_lr)

plt.figure(figsize=(5, 4))
sns.heatmap(cm_lr, annot=True, fmt="d", cmap="Blues")
plt.title("Confusion Matrix ‚Äî Logistic Regression")
plt.xlabel("Predicted label")
plt.ylabel("True label")
plt.show()


# In[37]:


cm_cb = confusion_matrix(y_test, y_pred_cb)

plt.figure(figsize=(5, 4))
sns.heatmap(cm_cb, annot=True, fmt="d", cmap="Blues")
plt.title("Confusion Matrix ‚Äî CatBoostClassifier")
plt.xlabel("Predicted label")
plt.ylabel("True label")
plt.show()


# In[38]:


cm_nn = confusion_matrix(y_test, y_pred_nn)

plt.figure(figsize=(5, 4))
sns.heatmap(cm_nn, annot=True, fmt="d", cmap="Blues")
plt.title("Confusion Matrix ‚Äî Neural Network")
plt.xlabel("Predicted label")
plt.ylabel("True label")
plt.show()


# –ò—Å—Ö–æ–¥—è –∏–∑ –º–µ—Ç—Ä–∏–∫, –ª—É—á—à–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏ —Å—Ç–∞–ª–∏ CatboostClassifier –∏ –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å. –ó–∞–π–º–µ–º—Å—è –ø–æ–¥–±–æ—Ä–æ–º –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è —ç—Ç–∏—Ö –º–æ–¥–µ–ª–µ–π –∏ —Å—Ä–∞–≤–Ω–∏–º –∏—Ö —Å–Ω–æ–≤–∞.

# ## CatBoostClassifier —Å –ø–æ–¥–æ–±—Ä–∞–Ω–Ω—ã–º–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
# 
# –î–ª—è –ø–æ–¥–±–æ—Ä–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ Catboost –Ω—É–∂–Ω–æ –æ—Ç–∫–∞—Ç–∏—Ç—å—Å—è –¥–æ scikit-learn==1.5.2
# 
# –ï—Å–ª–∏ –≤–æ–∑–Ω–∏–∫–∞—é—Ç –ø—Ä–æ–±–ª–µ–º—ã, –º–æ–∂–Ω–æ –ø—Ä–æ—Å—Ç–æ –∑–∞–º–µ–Ω–∏—Ç—å –º–æ–¥–µ–ª—å Catboost –Ω–∞ RandomForest, —Å –Ω–∏–º —Ç–∞–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º –Ω–µ –±—É–¥–µ—Ç.
# 
# –û–¥–Ω–∞–∫–æ, –¥–ª—è –Ω–∞—á–∞–ª–∞ –ª—É—á—à–µ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –±—É—Å—Ç–∏–Ω–≥, —Ç–∞–∫ –∫–∞–∫ –æ–Ω –≤—ã–¥–∞–µ—Ç –æ–±—ã—á–Ω–æ –∫–∞—á–µ—Å—Ç–≤–æ –ª—É—á—à–µ.

# In[39]:


get_ipython().system('pip install scikit-learn==1.5.2 -q')


# In[40]:


param_dist_cb_cls = {
    "depth": [4, 6, 8, 10],
    "learning_rate": [0.01, 0.03, 0.05, 0.1],
    "iterations": [300, 500, 800],
    "l2_leaf_reg": [1, 3, 5, 7, 9],
}

cb_cls = CatBoostClassifier(
    verbose=0,
    # task_type='GPU'
)

random_cb_cls = RandomizedSearchCV(
    cb_cls,
    param_distributions=param_dist_cb_cls,
    cv=3,
    # n_jobs=-1,
    verbose=1,
    n_iter=4,
)

random_cb_cls.fit(X_train, y_train)

best_cb_cls = random_cb_cls.best_estimator_
y_pred_cb = best_cb_cls.predict(X_test)
y_proba_cb = best_cb_cls.predict_proba(X_test)

print(f"Accuracy –¥–ª—è CatBoostClassifier: {accuracy_score(y_test, y_pred_cb):.3f}")
print(f"F1-macro –¥–ª—è CatBoostClassifier: {f1_score(y_test, y_pred_cb, average='macro'):.3f}")
print(f"Precision-macro –¥–ª—è CatBoostClassifier: {precision_score(y_test, y_pred_cb, average='macro'):.3f}")
print(f"Recall-macro –¥–ª—è CatBoostClassifier: {recall_score(y_test, y_pred_cb, average='macro'):.3f}")


# ## MLPClassifier —Å –ø–æ–¥–æ–±—Ä–∞–Ω–Ω—ã–º–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏

# In[41]:


param_grid_nn_cls = {
    "hidden_layer_sizes": [(64, 32), (128, 64)],
    "activation": ["relu", "tanh"],
    "learning_rate_init": [0.001, 0.01],
}

model_nn_cls = MLPClassifier(
)

random_nn_cls = RandomizedSearchCV(
    model_nn_cls,
    param_distributions=param_grid_nn_cls,
    cv=3,
    # n_jobs=-1,
    verbose=1,
    n_iter=3
)

random_nn_cls.fit(X_train, y_train)

best_nn_cls = random_nn_cls.best_estimator_
y_pred_nn = best_nn_cls.predict(X_test)
y_proba_nn = best_nn_cls.predict_proba(X_test)

print(f"Accuracy –¥–ª—è MLPClassifier: {accuracy_score(y_test, y_pred_nn):.3f}")
print(f"F1-macro –¥–ª—è MLPClassifier: {f1_score(y_test, y_pred_nn, average='macro'):.3f}")
print(f"Precision-macro –¥–ª—è MLPClassifier: {precision_score(y_test, y_pred_nn, average='macro'):.3f}")
print(f"Recall-macro –¥–ª—è MLPClassifier: {recall_score(y_test, y_pred_nn, average='macro'):.3f}")


# –õ—É—á—à–µ–π –º–æ–¥–µ–ª—å—é —Å –ø–æ–¥–æ–±—Ä–∞–Ω–Ω—ã–º–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ —Å—Ç–∞–ª–∞ CatBoostClassifier, –¥–ª—è –Ω–µ–µ –ø–æ—Å—Ç—Ä–æ–∏–º ROC-AUC.

# ## –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ ROC-AUC

# In[46]:


# –ú–∞—Å—Å–∏–≤ –∫–ª–∞—Å—Å–æ–≤
classes = np.unique(y_test)

# –ë–∏–Ω–∞—Ä–∏–∑—É–µ–º y_test
y_test_bin = label_binarize(y_test, classes=classes)

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ CatBoost
y_proba = best_cb_cls.predict_proba(X_test)

plt.figure(figsize=(8, 6))

# –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ ROC-–∫—Ä–∏–≤–æ–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞ –ø–æ –æ—Ç–¥–µ–ª—å–Ω–æ—Å—Ç–∏
for i, cls in enumerate(classes):
    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_proba[:, i])
    roc_auc = auc(fpr, tpr)

    plt.plot(fpr, tpr, lw=2, label=f"Class {cls} (AUC = {roc_auc:.3f})")

# –õ–∏–Ω–∏—è —Å–ª—É—á–∞–π–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
plt.plot([0, 1], [0, 1], "k--", lw=1)

plt.xlabel("False Positive Rate (FPR)")
plt.ylabel("True Positive Rate (TPR)")
plt.title("ROC Curves ‚Äî CatBoostClassifier (OvR)")
plt.legend(loc="lower right")
plt.grid(True)
plt.show()


# –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞ –º–µ—Ç—Ä–∏–∫–∞ –≤—ã—Å–æ–∫–∞—è. –ú–æ–¥–µ–ª—å —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è –æ—á–µ–Ω—å —Ö–æ—Ä–æ—à–æ.

# ## –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π

# In[47]:


import pandas as pd

proba_df = pd.DataFrame(y_proba, columns=[f"proba_{cls}" for cls in classes])

plt.figure(figsize=(10, 5))
sns.histplot(proba_df, kde=True, bins=25)
plt.title("Distribution of Predicted Probabilities ‚Äî CatBoostClassifier")
plt.xlabel("Probability")
plt.ylabel("Count")
plt.show()


# ## –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
# 
# –î–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π sklearn –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∏–±–ª–∏–æ—Ç–µ–∫—É joblib. –•–æ—Ç—è –¥–ª—è catboost-–º–æ–¥–µ–ª–µ–π —ç—Ç–æ—Ç –º–µ—Ç–æ–¥ —Ç–æ–∂–µ –ø–æ–¥—Ö–æ–¥–∏—Ç, –ª—É—á—à–µ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–æ–¥–µ–ª—å —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏.

# In[48]:


best_cb_cls.save_model('catboost_model_classification.cbm', format='cbm')

joblib.dump(best_nn_cls, 'nn_model_classification.pkl')


# ## –§—É–Ω–∫—Ü–∏—è –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è
# 
# –ß–∞—Å—Ç–æ –Ω–∞ —á–µ–º–ø–∏–æ–Ω–∞—Ç–∞—Ö –ø—Ä–æ—Å—è—Ç –æ—Ä–≥–∞–Ω–∏–∑–æ–≤–∞—Ç—å –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é AirFlow. 
# 
# –°–æ–∑–¥–∞–¥–∏–º —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—É—é –º–æ–∂–Ω–æ –±—É–¥–µ—Ç –ø–µ—Ä–µ–Ω–µ—Å—Ç–∏ –≤ Airflow –∏–ª–∏ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –≤ –±—É–¥—É—â–µ–º.
# 
# –ê–Ω–∞–ª–æ–≥–∏—á–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –º–æ–∂–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏. 

# In[56]:


def fine_tuning_regression(new_data: pd.DataFrame) -> dict:
    """
    –ö–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π fine-tuning —Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (MLPRegressor + CatBoostRegressor).
    –ú–æ–¥–µ–ª–∏ –ø–µ—Ä–µ–æ–±—É—á–∞—é—Ç—Å—è –Ω–∞ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–∏ —Å—Ç–∞—Ä—ã—Ö + –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö,
    –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ—Ç –∂–µ scaler –∏ —Ç–æ—Ç –∂–µ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π test –∏–∑ joblib.
    """

    # -------------------------------------------------
    # 1. –ó–ê–ì–†–£–ó–ö–ê –°–¢–ê–†–´–• –ê–†–¢–ï–§–ê–ö–¢–û–í
    # -------------------------------------------------
    scaler = joblib.load('scaler.pkl')
    old_X = joblib.load('old_X_train_reg.pkl')
    old_y = joblib.load('old_y_train_reg.pkl')

    cb_model = CatBoostRegressor()
    cb_model.load_model('catboost_model_regression.cbm')

    nn_model = joblib.load('nn_model_regression.pkl')

    # -------------------------------------------------
    # 2. –ü–û–î–ì–û–¢–û–í–ö–ê –ù–û–í–´–• –î–ê–ù–ù–´–•
    # -------------------------------------------------
    if 'AQI' not in new_data.columns:
        raise ValueError("–í new_data –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å—Ç–æ–ª–±–µ—Ü 'AQI'.")

    X_new = new_data.drop('AQI', axis=1)
    y_new = new_data['AQI'].astype(float)

    # –ø—Ä–∏–º–µ–Ω—è–µ–º —Ç–æ—Ç –∂–µ scaler, —á—Ç–æ –∏ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏
    X_new_scaled = scaler.transform(X_new)

    # -------------------------------------------------
    # 3. –û–ë–™–ï–î–ò–ù–ï–ù–ò–ï –°–¢–ê–†–´–• + –ù–û–í–´–• –î–ê–ù–ù–´–•
    # -------------------------------------------------
    X_full = np.vstack([old_X, X_new_scaled])
    y_full = np.concatenate([old_y, y_new])

    # -------------------------------------------------
    # 4. –ü–û–í–¢–û–†–ù–û–ï –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô
    # -------------------------------------------------
    nn_model.fit(X_full, y_full)
    cb_model.fit(X_full, y_full)

    # -------------------------------------------------
    # 5. –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ù–ê –ó–ê–ì–†–£–ñ–ï–ù–ù–û–ú (joblib) TEST –ù–ê–ë–û–†–ï
    # -------------------------------------------------
    X_test_scaled = joblib.load('X_test_reg.pkl')
    y_test = joblib.load('y_test_reg.pkl')

    y_pred_cb = cb_model.predict(X_test_scaled)
    y_pred_nn = nn_model.predict(X_test_scaled)

    # -------------------------------------------------
    # 6. –†–ê–°–ß–Å–¢ –ú–ï–¢–†–ò–ö
    # -------------------------------------------------
    metrics = {
        "CatBoostRegressor": {
            "MAE": round(mean_absolute_error(y_test, y_pred_cb), 3),
            "RMSE": round(root_mean_squared_error(y_test, y_pred_cb), 3),
            "R2": round(r2_score(y_test, y_pred_cb), 3),
        },
        "MLPRegressor": {
            "MAE": round(mean_absolute_error(y_test, y_pred_nn), 3),
            "RMSE": round(root_mean_squared_error(y_test, y_pred_nn), 3),
            "R2": round(r2_score(y_test, y_pred_nn), 3),
        }
    }

    # -------------------------------------------------
    # 7. –°–û–•–†–ê–ù–ï–ù–ò–ï –û–ë–ù–û–í–õ–Å–ù–ù–´–• –ú–û–î–ï–õ–ï–ô –ò TRAIN –î–ê–ù–ù–´–•
    # -------------------------------------------------
    cb_model.save_model('catboost_model_regression.cbm')
    joblib.dump(nn_model, 'nn_model_regression.pkl')

    joblib.dump(X_full, 'old_X_train_reg.pkl')
    joblib.dump(y_full, 'old_y_train_reg.pkl')

    return metrics


# In[57]:


# –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é

new_data = X_train.copy()
new_data['AQI'] = y_train
fine_tuning_regression(new_data)

